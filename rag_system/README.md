# LaTeX RAG System - Design Documentation

## ğŸ¯ Project Overview

This project implements a Retrieval-Augmented Generation (RAG) system specifically designed for creating knowledge bases from LaTeX documents. The system is built with modularity and reusability in mind, using modern AI/ML technologies and best practices.

## ğŸ—ï¸ System Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document       â”‚    â”‚   Embedding     â”‚    â”‚   Vector        â”‚
â”‚  Processor      â”‚â”€â”€â”€â–¶â”‚   Engine        â”‚â”€â”€â”€â–¶â”‚   Database      â”‚
â”‚  (LaTeX)        â”‚    â”‚  (Sentence      â”‚    â”‚  (Weaviate)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Transformers) â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  Generation     â”‚    â”‚   Retrieval     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  System         â”‚â—€â”€â”€â”€â”‚   System        â”‚
â”‚  (Ollama +      â”‚    â”‚  (Hybrid        â”‚
â”‚   Mistral 7B)   â”‚    â”‚   Search)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
rag_system/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_preprocessor.py    # LaTeX parsing and preprocessing
â”‚   â”œâ”€â”€ data_chunker.py            # Format-agnostic chunking
â”‚   â”œâ”€â”€ embedding_engine.py        # Vector embeddings
â”‚   â”œâ”€â”€ vector_store.py            # Weaviate database interface
â”‚   â”œâ”€â”€ retriever.py               # Hybrid search and retrieval
â”‚   â””â”€â”€ generator.py               # Answer generation with Ollama
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ latex_parser.py            # LaTeX-specific utilities
â”‚   â”œâ”€â”€ text_processing.py         # Text cleaning and preprocessing
â”‚   â””â”€â”€ config_validator.py        # Configuration validation
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                # Configuration management
â”‚   â””â”€â”€ default_config.yaml        # Default configuration
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sample_queries.py          # Example usage
â”‚   â””â”€â”€ latex_samples/             # Sample LaTeX files for testing
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_document_processor.py
â”‚   â”œâ”€â”€ test_data_chunker.py
â”‚   â”œâ”€â”€ test_embedding_engine.py
â”‚   â”œâ”€â”€ test_vector_store.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â””â”€â”€ test_generator.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ design_decisions.md        # This file
â”‚   â”œâ”€â”€ api_reference.md           # API documentation
â”‚   â””â”€â”€ deployment_guide.md        # Deployment instructions
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ setup.py                       # Package setup
â””â”€â”€ README.md                      # Main project README
```

## ğŸ¯ Design Decisions

### A. Document Processing Strategy

#### LaTeX Handling
- **Equation Preservation**: Keep mathematical equations intact while removing other LaTeX commands
- **Citation Strategy**: Store citations as separate database entries with rich metadata
- **Command Removal**: Strip LaTeX formatting commands (`\section{}`, `\textbf{}`, etc.) while preserving semantic content

#### Chunking Strategy
- **Adaptive Fixed-Size**: 768 tokens with line boundary respect
- **Overlap**: 100-150 tokens between chunks for context preservation
- **Object-Oriented Design**: Configurable `DataChunker` class for format-agnostic flexibility

#### Citation Metadata Structure
```python
{
    "type": "citation",
    "author": "Einstein, A.",
    "year": 1905,
    "title": "On the Electrodynamics of Moving Bodies",
    "doi": "10.1002/andp.19053221004",
    "content": "The theory of relativity...",
    "source_document": "chapter_1.tex",
    "page_reference": 15,
    "chunk_id": "chunk_001"
}
```

### B. Embedding & Storage

#### Embedding Model
- **Technology**: Sentence Transformers (local, free)
- **Model**: `all-mpnet-base-v2` (768 dimensions, good for technical content)
- **Alternative**: `multi-qa-MiniLM-L6-v2` (optimized for Q&A)

#### Vector Database
- **Technology**: Weaviate
- **Integration**: Built-in `text2vec-transformers` module
- **Features**: Rich querying, filtering, and metadata support

#### Configuration
```python
# Weaviate with Sentence Transformers
{
    "vectorizer": "text2vec-transformers",
    "moduleConfig": {
        "text2vec-transformers": {
            "model": "all-mpnet-base-v2",
            "poolingStrategy": "masked_mean",
            "vectorizeClassName": False
        }
    }
}
```

### C. Retrieval Strategy

#### Hybrid Search
- **Vector Search**: Dense embeddings for semantic similarity
- **BM25 Search**: Sparse keyword matching for exact terms
- **Configurable Weights**: Adjustable alpha parameter (0.0-1.0)
- **Fusion Type**: Relative score combination

#### Query Processing
- **Technical Term Handling**: Preserve mathematical notation and technical terms
- **Citation Queries**: Special handling for author/year queries
- **Equation Queries**: Support for mathematical concept searches

### D. Generation System

#### LLM Configuration
- **Technology**: Ollama + Mistral 7B
- **Deployment**: Local, free
- **Context Window**: 4096 tokens
- **Specialization**: Good performance on technical/scientific content

#### Prompt Engineering
- **RAG-Specific**: Structured prompts for context-aware generation
- **Citation Integration**: Include citation metadata in responses
- **Equation Handling**: Preserve mathematical notation in outputs

## ğŸ”§ Implementation Plan

### Phase 1: Core Infrastructure
1. Project structure setup
2. Configuration management
3. Base classes and interfaces
4. Unit testing framework

### Phase 2: Document Processing
1. LaTeX parser implementation
2. Data chunker implementation
3. Citation extraction
4. Metadata handling

### Phase 3: Embedding & Storage
1. Weaviate integration
2. Sentence Transformers setup
3. Document indexing
4. Vector storage optimization

### Phase 4: Retrieval System
1. Hybrid search implementation
2. Query preprocessing
3. Result ranking
4. Performance optimization

### Phase 5: Generation System
1. Ollama integration
2. Prompt engineering
3. Response formatting
4. Quality evaluation

### Phase 6: Integration & Testing
1. End-to-end testing
2. Performance benchmarking
3. Documentation completion
4. Deployment preparation

## ğŸš€ Getting Started

### Prerequisites
- Python 3.11+
- Docker (for Weaviate)
- Ollama (for Mistral 7B)
- Git

### Installation
```bash
# Clone the repository
git clone <repository-url>
cd rag_system

# Install dependencies
pip install -r requirements.txt

# Start Weaviate
docker run -d --name weaviate -p 8080:8080 semitechnologies/weaviate:1.22.4

# Install Ollama and Mistral 7B
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull mistral:7b
```

### Quick Start
```python
from rag_system.core import RAGSystem
from rag_system.config import ChunkConfig

# Initialize the system
config = ChunkConfig(chunk_size=768, overlap=100)
rag = RAGSystem(config)

# Process a LaTeX document
rag.process_document("path/to/document.tex")

# Query the knowledge base
response = rag.query("What is the main equation in chapter 2?")
print(response)
```

## ğŸ“Š Performance Considerations

### Chunk Size Optimization
- **Default**: 768 tokens
- **Testing Range**: 512-1024 tokens
- **Optimization**: Empirical testing with domain-specific queries

### Embedding Quality
- **Model Selection**: Balance between speed and quality
- **Fine-tuning**: Consider domain-specific fine-tuning for better performance
- **Evaluation**: Use semantic similarity metrics for validation

### Retrieval Performance
- **Hybrid Weights**: Optimize alpha parameter for your use case
- **Caching**: Implement result caching for common queries
- **Indexing**: Optimize Weaviate schema for your query patterns

## ğŸ” Evaluation Metrics

### Retrieval Quality
- **Precision@K**: Top-K retrieval accuracy
- **Recall@K**: Coverage of relevant documents
- **MRR**: Mean Reciprocal Rank for ranking quality

### Generation Quality
- **ROUGE**: Text similarity metrics
- **BLEU**: Translation quality metrics
- **Human Evaluation**: Manual assessment of answer quality

### System Performance
- **Latency**: Query response time
- **Throughput**: Queries per second
- **Resource Usage**: Memory and CPU utilization

## ğŸ¤ Contributing

### Development Guidelines
1. Follow PEP 8 coding standards
2. Write comprehensive unit tests
3. Document all public APIs
4. Use type hints throughout
5. Implement proper error handling

### Testing Strategy
- **Unit Tests**: Individual component testing
- **Integration Tests**: Component interaction testing
- **End-to-End Tests**: Complete workflow testing
- **Performance Tests**: Load and stress testing

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Weaviate team for the excellent vector database
- Sentence Transformers for the embedding models
- Ollama team for the local LLM framework
- The open-source AI/ML community for inspiration and tools
